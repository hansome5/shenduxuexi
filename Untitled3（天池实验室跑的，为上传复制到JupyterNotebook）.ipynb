{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b8a11-5a32-4a29-aba3-b0aa07290030",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf250909-29fa-49fc-81d0-0db4bbb803f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9b95a-b833-4f34-bfcc-3cdbfd0ef3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.init import xavier_normal_, constant_\n",
    "import torch.utils.data as D\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,KFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d67c89-7c00-4e6b-b9db-850b01e107b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数配置\n",
    "\n",
    "config = {\n",
    "    \"train_path\":'/mnt/workspace/data/used_car_train_20200313.csv',\n",
    "    \"test_path\":'/mnt/workspace/data/used_car_testA_20200313.csv',\n",
    "    \"epoch\" : 5,\n",
    "    \"batch_size\" : 512,\n",
    "    \"lr\" : 0.001,\n",
    "    \"model_ckpt_dir\":'./',\n",
    "   # \"device\" : 'cuda:0', # 'cpu'\n",
    "    \"device\" :torch.device('cpu'),\n",
    "    \"num_cols\" : ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14'],\n",
    "    \"cate_cols\" : ['model','brand','bodyType','fuelType','gearbox','seller','notRepairedDamage']\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"is_use_cate_cols\" : True,\n",
    "    \"embedding_dim\" : 4,\n",
    "    \"hidden_units\" : [256,128,64,32]\n",
    "}\n",
    "model_config[\"num_cols\"] = config['num_cols']\n",
    "model_config['cate_cols'] = config['cate_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea59643-132a-486c-8f13-e7599c5dfd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['train_path'],sep=' ')\n",
    "test_df = pd.read_csv(config['test_path'],sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f31699-a2b1-475a-afa5-839f202b2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f4ae1-c923-4e73-9ffb-cdb00054d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881e105-fdb7-48e9-9c3b-4b63f68f6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征异常值简单处理\n",
    "df.loc[df['power']>600,'power'] = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b09889-8a16-4d00-be07-777755982f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征\n",
    "for col in config['num_cols']:\n",
    "    # 绘制密度图\n",
    "    sns.kdeplot(df[col], fill=True)\n",
    "\n",
    "    # 设置图形标题和标签\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7249e-45ce-4033-9e08-dd4f58f1e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散特征\n",
    "for col in config['cate_cols']:\n",
    "    # 统计特征频次\n",
    "    counts = df[col].value_counts()\n",
    "\n",
    "    # 绘制条形图\n",
    "    counts.plot(kind='bar')\n",
    "\n",
    "    # 设置图形标题和标签\n",
    "    plt.title(f'{col} Frequencies')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342043d7-e382-46a7-b171-869d7c90a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散特征\n",
    "for col in config['cate_cols']:\n",
    "    # 统计特征频次\n",
    "    counts = df[col].value_counts()\n",
    "\n",
    "    # 绘制条形图\n",
    "    counts.plot(kind='bar')\n",
    "\n",
    "    # 设置图形标题和标签\n",
    "    plt.title(f'{col} Frequencies')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291444a-89bc-4097-a514-05bba81087d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散特征编码\n",
    "vocab_map = defaultdict(dict)\n",
    "for col in tqdm(config['cate_cols']):\n",
    "    df[col] = df[col].fillna('-1')\n",
    "    map_dict = dict(zip(df[col].unique(), range(df[col].nunique())))\n",
    "    # label enc\n",
    "    df[col] = df[col].map(map_dict)\n",
    "    vocab_map[col]['vocab_size'] = len(map_dict)\n",
    "model_config['vocab_map'] = vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b128520-7690-42f8-b1be-fbd40097b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['vocab_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5164ccc-2040-439e-b8b7-0018c894a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#连续特征编码\n",
    "for col in config['num_cols']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "    df[col] = (df[col]-df[col].min()) / (df[col].max()-df[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfd3cd-0008-497f-9ff7-cd275da6f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['price'].notna()].reset_index(drop=True)\n",
    "# 标签范围太大不利于神经网络进行拟合，这里先对其进行log变换\n",
    "train_df['price'] = np.log(train_df['price'])\n",
    "test_df = df[df['price'].isna()].reset_index(drop=True)\n",
    "del test_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf8b64-fab3-4f66-9daf-bbf919a2a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset构造\n",
    "class SaleDataset(Dataset):\n",
    "    def __init__(self,df,cate_cols,num_cols):\n",
    "        self.df = df\n",
    "        self.feature_name = cate_cols + num_cols\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        data = dict()\n",
    "        for col in self.feature_name:\n",
    "            data[col] = torch.Tensor([self.df[col].iloc[index]]).squeeze(-1)\n",
    "        if 'price' in self.df.columns:\n",
    "            data['price'] = torch.Tensor([self.df['price'].iloc[index]]).squeeze(-1)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_dataloader(df, cate_cols ,num_cols, batch_size=256, num_workers=2, shuffle=True):\n",
    "    dataset = SaleDataset(df,cate_cols,num_cols)\n",
    "    dataloader = D.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229745cc-c4d3-4d4c-b205-7fb7ab8bd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "train_dataset = SaleDataset(train_df,config['cate_cols'],config['num_cols'])\n",
    "train_dataset.__getitem__(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40c2ac-70cc-4ff3-909a-c811ce2dda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user进行Embedding化，有 4个 user，我想把每个user编码成一个8维的向量\n",
    "num_user = 4\n",
    "emb_dim = 8\n",
    "user_emb_layer = nn.Embedding(num_user,emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc2283-75b7-49ab-9ed8-281eb8f898eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112a2c-4d34-44d3-b312-1e07da6e2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = torch.Tensor([0,2]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dec2ee-a136-4b02-a6be-f0b281508e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_layer(query_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa7e35-7d16-490e-8209-2f8bcbcbb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding层：用于对离散特征进行编码映射\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_map = None,\n",
    "                 embedding_dim = None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.vocab_map = vocab_map\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = nn.ModuleDict()\n",
    "\n",
    "        self.emb_feature = []\n",
    "        # 使用字典来存储每个离散特征的Embedding标\n",
    "        for col in self.vocab_map.keys():\n",
    "            self.emb_feature.append(col)\n",
    "            self.embedding_layer.update({col : nn.Embedding(\n",
    "                self.vocab_map[col]['vocab_size'],\n",
    "                self.embedding_dim,\n",
    "            )})\n",
    "\n",
    "    def forward(self, X):\n",
    "        #对所有的sparse特征挨个进行embedding\n",
    "        feature_emb_list = []\n",
    "        for col in self.emb_feature:\n",
    "            inp = X[col].long().view(-1, 1)\n",
    "            feature_emb_list.append(self.embedding_layer[col](inp))\n",
    "        return torch.cat(feature_emb_list,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e1c24-d829-4ceb-8397-a92940e01f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim=None,\n",
    "                 hidden_units=[],\n",
    "                 hidden_activations=\"ReLU\",\n",
    "                 final_activation=None,\n",
    "                 dropout_rates=0,\n",
    "                 batch_norm=False,\n",
    "                 use_bias=True):\n",
    "        super(MLP, self).__init__()\n",
    "        dense_layers = []\n",
    "        if not isinstance(dropout_rates, list):\n",
    "            dropout_rates = [dropout_rates] * len(hidden_units)\n",
    "        if not isinstance(hidden_activations, list):\n",
    "            hidden_activations = [hidden_activations] * len(hidden_units)\n",
    "        hidden_activations = [self.set_activation(x) for x in hidden_activations]\n",
    "        hidden_units = [input_dim] + hidden_units\n",
    "        for idx in range(len(hidden_units) - 1):\n",
    "            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n",
    "            if batch_norm:\n",
    "                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n",
    "            if hidden_activations[idx]:\n",
    "                dense_layers.append(hidden_activations[idx])\n",
    "            if dropout_rates[idx] > 0:\n",
    "                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n",
    "        if output_dim is not None:\n",
    "            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n",
    "        if final_activation is not None:\n",
    "            dense_layers.append(set_activation(final_activation))\n",
    "        self.dnn = nn.Sequential(*dense_layers)  # * used to unpack list\n",
    "    \n",
    "    def set_activation(self,activation):\n",
    "        if isinstance(activation, str):\n",
    "            if activation.lower() == \"relu\":\n",
    "                return nn.ReLU()\n",
    "            elif activation.lower() == \"sigmoid\":\n",
    "                return nn.Sigmoid()\n",
    "            elif activation.lower() == \"tanh\":\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                return getattr(nn, activation)()\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.dnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416d61f-b562-4d49-b9dd-dab6ac418cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaleModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                is_use_cate_cols = True,\n",
    "                vocab_map = None,\n",
    "                embedding_dim = 16,\n",
    "                num_cols = None,\n",
    "                cate_cols = None,\n",
    "                hidden_units = [256,128,64,32],\n",
    "                loss_fun = 'nn.L1Loss()'):\n",
    "        super(SaleModel, self).__init__()\n",
    "        self.is_use_cate_cols = is_use_cate_cols\n",
    "        self.vocab_map = vocab_map\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_cols = num_cols\n",
    "        self.num_nums_fea = len(num_cols)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.loss_fun = eval(loss_fun) # self.loss_fun  = nn.L1Loss()\n",
    "        \n",
    "        if is_use_cate_cols:\n",
    "            self.emb_layer = EmbeddingLayer(vocab_map=vocab_map,embedding_dim=embedding_dim)\n",
    "            self.mlp = MLP(\n",
    "                 self.num_nums_fea + self.embedding_dim*len(vocab_map),\n",
    "                 output_dim=1,\n",
    "                 hidden_units=self.hidden_units,\n",
    "                 hidden_activations=\"ReLU\",\n",
    "                 final_activation=None,\n",
    "                 dropout_rates=0,\n",
    "                 batch_norm=True,\n",
    "                 use_bias=True)\n",
    "        else:\n",
    "            self.mlp = MLP(\n",
    "                 self.num_nums_fea,\n",
    "                 output_dim=1,\n",
    "                 hidden_units=self.hidden_units,\n",
    "                 hidden_activations=\"ReLU\",\n",
    "                 final_activation=None,\n",
    "                 dropout_rates=0,\n",
    "                 batch_norm=True,\n",
    "                 use_bias=True)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            xavier_normal_(module.weight.data)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            xavier_normal_(module.weight.data)\n",
    "        \n",
    "    def get_dense_input(self, data):\n",
    "        dense_input = []\n",
    "        for col in self.num_cols:\n",
    "            dense_input.append(data[col])\n",
    "        return torch.stack(dense_input,dim=-1)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        dense_fea = self.get_dense_input(data) # [batch,num_nums_cols]\n",
    "        if self.is_use_cate_cols:\n",
    "            sparse_fea = self.emb_layer(data) # [batch,num_cate_cols,emb]\n",
    "            sparse_fea = torch.flatten(sparse_fea,start_dim=1) # [batch,num_cate_cols*emb]\n",
    "            mlp_input = torch.cat([sparse_fea, dense_fea],axis=-1) # [batch,num_nums_cols+num_cate_cols*emb]\n",
    "        else:\n",
    "            mlp_input = dense_fea\n",
    "            \n",
    "        y_pred = self.mlp(mlp_input)\n",
    "        # 为了把复杂多变的loss计算对外不感知，所以写在forward里面\n",
    "        if 'price' in data.keys():\n",
    "            loss = self.loss_fun(y_pred.squeeze(),data['price'])\n",
    "            output_dict = {'pred':y_pred,'loss':loss}\n",
    "        else:\n",
    "            output_dict = {'pred':y_pred}\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347aee8-23ef-4f0b-bd0b-8778b114387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型，验证模型，这里就是八股文，熟悉基础pipeline\n",
    "def train_model(model, train_loader, optimizer, device, metric_list=['mean_absolute_error']):\n",
    "    model.train()\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    max_iter = int(train_loader.dataset.__len__() / train_loader.batch_size)\n",
    "    for idx,data in enumerate(train_loader):\n",
    "        # 把数据拷贝在指定的device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向+Loss计算\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "        loss = output['loss']\n",
    "        # 八股文完成模型权重更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "        label_list.extend(data['price'].squeeze(-1).cpu().detach().numpy())\n",
    "        \n",
    "        if idx%50==0:\n",
    "            logger.info(f\"Iter:{idx}/{max_iter} Loss:{round(loss.item(),4)}\")\n",
    "        \n",
    "    res_dict = dict()\n",
    "    for metric in metric_list:\n",
    "        res_dict[metric] = eval(metric)(label_list,pred_list)\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "def valid_model(model, valid_loader, device, metric_list=['mean_absolute_error']):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for data in (valid_loader):\n",
    "        # 把数据拷贝在指定的device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "\n",
    "        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "        label_list.extend(data['price'].squeeze(-1).cpu().detach().numpy())\n",
    "    \n",
    "    res_dict = dict()\n",
    "    for metric in metric_list:\n",
    "        res_dict[metric] = eval(metric)(label_list,pred_list)\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "\n",
    "    for data in test_loader:\n",
    "        # 把数据拷贝在指定的device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "        pred_list.extend(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "    return np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aceba-aa23-4518-bddc-e16d8e444a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_dataloader(test_df, config['cate_cols'] ,config['num_cols'], batch_size=config['batch_size'], num_workers=0, shuffle=False)\n",
    "    \n",
    "\n",
    "n_fold = 5\n",
    "oof_pre = np.zeros(len(train_df))\n",
    "y_pre = np.zeros(len(test_df))\n",
    "device = torch.device(config['device'])\n",
    "\n",
    "kf = KFold(n_splits=n_fold)\n",
    "for fold_, (trn_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    logger.info(f\"Fold {fold_+1}\")\n",
    "    temp_train_df = train_df.iloc[trn_idx].reset_index(drop=True)\n",
    "    temp_valid_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_loader = get_dataloader(temp_train_df, config['cate_cols'] ,config['num_cols'], batch_size=config['batch_size'], num_workers=4, shuffle=True)\n",
    "    valid_loader = get_dataloader(temp_valid_df, config['cate_cols'] ,config['num_cols'], batch_size=config['batch_size'], num_workers=0, shuffle=False)\n",
    "    #声明模型\n",
    "    model = SaleModel(**model_config)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #声明Trainer\n",
    "    for epoch in range(config['epoch']):\n",
    "        #模型训练\n",
    "        logger.info(f\"Start Training Epoch:{epoch+1}\")\n",
    "        train_metirc = train_model(model,train_loader,optimizer=optimizer,device=device)\n",
    "        logger.info(f\"Train Metric: {train_metirc}\")\n",
    "        #模型验证\n",
    "        valid_metric = valid_model(model,valid_loader,device)\n",
    "        logger.info(f\"Valid Metric: {valid_metric}\")\n",
    "    #保存模型权重和enc_dict\n",
    "    save_dict = {'model': model.state_dict()}\n",
    "    torch.save(save_dict, os.path.join(config['model_ckpt_dir'], f'model_{fold_}.pth'))\n",
    "    # oof推理\n",
    "    oof_pre[val_idx] = test_model(model, valid_loader, device=device)\n",
    "    # 测试集推理\n",
    "    y_pre += np.array(test_model(model, test_loader, device=device)) / n_fold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad1779-9724-4b49-bc68-14d52f6e0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际价格的预测结果\n",
    "oof_pre_ori = np.exp(oof_pre)\n",
    "price_ori = np.exp(train_df['price'])\n",
    "mean_absolute_error(price_ori,oof_pre_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98154a-98e4-440f-986b-013e564a3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df =pd.DataFrame()\n",
    "res_df['SaleID'] = test_df['SaleID']\n",
    "res_df['price'] = np.exp(y_pre)\n",
    "res_df.to_csv('torch_baseline.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
